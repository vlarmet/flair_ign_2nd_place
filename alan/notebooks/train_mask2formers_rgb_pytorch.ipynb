{"cells":[{"cell_type":"markdown","source":["# FINETUNE MASK2FORMER RGB (pytorch)\n","\n","\n","---\n","\n","\n","<a target=\"_blank\" href=\"https://colab.research.google.com/drive/1rcNLoi7bTRwwIQKjNmOIMLEVBV2cLB9O\">\n","  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n","</a>\n","\n","https://huggingface.co/docs/transformers/model_doc/mask2former"],"metadata":{"id":"Eb1acyvs9WtY"}},{"cell_type":"markdown","source":["<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/mask2former_architecture.jpg\"  width=\"600\">\n","\n","\n"],"metadata":{"id":"lug7iO6MLg5a"}},{"cell_type":"markdown","source":["## Install dependencies\n","\n","---\n","\n"],"metadata":{"id":"BCe7Md3C-jNJ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"nUnQMNXaCiel"},"outputs":[],"source":["!pip install split-folders\n","!pip install git+https://github.com/huggingface/transformers.git@main\n","!pip install -q evaluate\n","!pip install rasterio"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A2a23Q-xfPgY"},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","from transformers import AdamW\n","import torch\n","from torch import nn\n","from sklearn.metrics import accuracy_score\n","from tqdm.notebook import tqdm\n","import os\n","from PIL import Image\n","from transformers import MaskFormerImageProcessor, Mask2FormerForUniversalSegmentation\n","import pandas as pd\n","import cv2\n","import numpy as np\n","import albumentations as aug\n","import rasterio\n","from pathlib import Path\n","import splitfolders\n","import shutil"]},{"cell_type":"markdown","source":["## Check GPU Ressources\n","\n","---\n","\n"],"metadata":{"id":"Y6tWQpu3925Y"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"OAFPjim0kGYQ"},"outputs":[],"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"]},{"cell_type":"markdown","source":["## Connect do GoogleDrive\n","\n","---\n"],"metadata":{"id":"2m55jdRp9_Co"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ruJuqkf7EW_M"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"markdown","source":["## Unzip training data\n","\n","---\n","\n"],"metadata":{"id":"XL6RZox_-PRX"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"gW8ZmnThPD8n"},"outputs":[],"source":["!unzip /content/gdrive/MyDrive/flair-one/data/flair-one_train.zip"]},{"cell_type":"markdown","metadata":{"id":"csO0MuKbocXf"},"source":["## Split data for train/val\n","\n","---\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"05ZgYvr1gIJS"},"outputs":[],"source":["! mkdir \"/content/temp\"\n","! mkdir \"/content/data\"\n","! mkdir \"/content/data/masks\"\n","! mkdir \"/content/data/images\"\n","\n","# Chemin du dossier source\n","src_folder = '/content/train/'\n","\n","# Chemin du dossier de destination\n","dst_folder = '/content/temp/'\n","\n","for subdir, dirs, files in os.walk(src_folder):\n","    for file in files:\n","        src_file = os.path.join(subdir, file)\n","        dst_file = os.path.join(dst_folder, file)\n","        shutil.move(src_file, dst_folder)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zkw1czotBqJB"},"outputs":[],"source":["cd temp"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NaIKB7Z85Vs5"},"outputs":[],"source":["!mv MSK*.tif /content/data/masks/\n","!mv  IMG*.tif /content/data/images/\n","\n","splitfolders.ratio(\"/content/data/\", seed=1337, ratio=(.99, .01), move=True) # default values\n","\n","# remove image with all pixels are None classe\n","!rm /content/temp/output/train/images/IMG_054774.tif\n","!rm /content/temp/output/train/masks/MSK_054774.tif\n","\n","!rm /content/temp/output/train/images/IMG_054792.tif\n","!rm /content/temp/output/train/masks/MSK_054792.tif"]},{"cell_type":"markdown","source":["## Define a class for the image segmentation dataset\n","\n","---\n","\n"],"metadata":{"id":"wp69s4kc-xH3"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cxfdkul3YCei"},"outputs":[],"source":["def get_data_paths (path, filter):\n","    for path in Path(path).rglob(filter):\n","        yield path.resolve().as_posix()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ie3wHlDxfdM4"},"outputs":[],"source":["class ImageSegmentationDataset(Dataset):\n","    \"\"\"Image segmentation dataset.\"\"\"\n","\n","    def __init__(self, root_dir, transforms=None, train=True):\n","        \"\"\"\n","        Args:\n","            root_dir (string): Root directory of the dataset containing the images + annotations.\n","            feature_extractor (SegFormerFeatureExtractor): feature extractor to prepare images + segmentation maps.\n","            train (bool): Whether to load \"training\" or \"validation\" images + annotations.\n","        \"\"\"\n","        self.root_dir = root_dir\n","        self.train = train\n","        self.transforms = transforms\n","\n","        self.images = sorted(list(get_data_paths(Path(self.root_dir), 'IMG*.tif')), key=lambda x: int(x.split('_')[-1][:-4]))\n","        self.masks = sorted(list(get_data_paths(Path(self.root_dir), 'MSK*.tif')), key=lambda x: int(x.split('_')[-1][:-4]))\n","\n","        assert len(self.images) == len(self.masks), \"There must be as many images as there are segmentation maps\"\n","      \n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        \n","        image_file = self.images[idx]\n","        with rasterio.open(image_file) as src_img:\n","            original_image = src_img.read([1,2,3]).swapaxes(0, 2).swapaxes(0, 1)\n","\n","        \n","        mask_file = self.masks[idx]\n","        with rasterio.open(mask_file) as src_msk:\n","            original_segmentation_map = src_msk.read()[0]\n","        original_segmentation_map = np.squeeze(original_segmentation_map)\n","        original_segmentation_map[original_segmentation_map > 12] = 0\n","\n","        \n","        transformed = self.transforms(image=original_image, mask=original_segmentation_map)\n","        image, segmentation_map = transformed['image'], transformed['mask']\n","\n","        # convert to C, H, W\n","        image = image.transpose(2,0,1)\n","\n","        return image, segmentation_map, original_image, original_segmentation_map"]},{"cell_type":"markdown","source":["## Data augmentation with albumentation\n","\n","---\n","\n"],"metadata":{"id":"MehPmMAgACNY"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"oP6CeJLs_fux"},"outputs":[],"source":["MEAN = np.array([0.44050665, 0.45704361, 0.42254708]) \n","STD = np.array([0.20264351, 0.1782405 , 0.17575739]) \n","\n","train_transform = aug.Compose([\n","    aug.VerticalFlip(p=0.5),\n","    aug.HorizontalFlip(p=0.5),\n","    aug.RandomRotate90(p=0.5),\n","    aug.ColorJitter(),\n","    aug.RandomBrightnessContrast(),\n","    aug.Normalize(mean=MEAN, std=STD),\n","])\n","\n","test_transform = aug.Compose([\n","    aug.Normalize(mean=MEAN, std=STD),\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sFvvSI4ILMl6"},"outputs":[],"source":["root_train = '/content/temp/output/train'\n","root_val = '/content/temp/output/val'\n","\n","train_dataset = ImageSegmentationDataset(root_dir=root_train,  transforms=train_transform)\n","valid_dataset = ImageSegmentationDataset(root_dir=root_val, transforms=test_transform, train=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_6BnOrCOLMl6"},"outputs":[],"source":["print(\"Number of training examples:\", len(train_dataset))\n","print(\"Number of validation examples:\", len(valid_dataset))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JqIAYLwv7MIS"},"outputs":[],"source":["from transformers import MaskFormerImageProcessor\n","\n","# Create a preprocessor\n","preprocessor = MaskFormerImageProcessor(ignore_index=0, reduce_labels=False, do_resize=False, do_rescale=False, do_normalize=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_NtwFymK7MPx"},"outputs":[],"source":["from torch.utils.data import DataLoader\n","\n","def collate_fn(batch):\n","    inputs = list(zip(*batch))\n","    images = inputs[0]\n","    segmentation_maps = inputs[1]\n","    # this function pads the inputs to the same size,\n","    # and creates a pixel mask\n","    # actually padding isn't required here since we are cropping\n","    batch = preprocessor(\n","        images,\n","        segmentation_maps=segmentation_maps,\n","        return_tensors=\"pt\",\n","    )\n","\n","    batch[\"original_images\"] = inputs[2]\n","    batch[\"original_segmentation_maps\"] = inputs[3]\n","    \n","    return batch\n","\n","train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n","test_dataloader = DataLoader(valid_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n","\n","batch = next(iter(train_dataloader))\n","for k,v in batch.items():\n","  if isinstance(v, torch.Tensor):\n","    print(k,v.shape)\n","  else:\n","    print(k,v[0].shape)"]},{"cell_type":"markdown","source":["## Classes metadata\n","\n","---\n","\n"],"metadata":{"id":"RvRT_GSrALEM"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"A-MvayrKhGFC"},"outputs":[],"source":["classes = ['None','building','pervious surface','impervious surface','bare soil','water','coniferous','deciduous','brushwood','vineyard','herbaceous vegetation','agricultural land','plowed land']\n","id2label = classes.to_dict()\n","label2id = {v: k for k, v in id2label.items()}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UTz_nRV97MWZ"},"outputs":[],"source":["pixel_values = batch[\"pixel_values\"][0].numpy()\n","pixel_values.shape"]},{"cell_type":"markdown","source":["# Fine-tune a Mask2former model\n","\n","---\n"],"metadata":{"id":"XsyUuvnaApEY"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"v9brkvSh7MZh"},"outputs":[],"source":["from transformers import Mask2FormerForUniversalSegmentation\n","\n","# Replace the head of the pre-trained model\n","model = Mask2FormerForUniversalSegmentation.from_pretrained(\"facebook/mask2former-swin-base-IN21k-ade-semantic\",\n","                                                          id2label=id2label,\n","                                                          ignore_mismatched_sizes=True)\n","     "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8QnFLPlG74mV"},"outputs":[],"source":["import evaluate\n","metric = evaluate.load(\"mean_iou\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DmMG_ZKvCbj6"},"outputs":[],"source":["# if you have a pretrained checkpoint...\n","# model.load_state_dict(torch.load(\"/content/gdrive/MyDrive/flair-one/models/mask2former-swin-base-ade-semantic/mask2former-swin-base-ade-semantic\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MvmTeN8x74pu"},"outputs":[],"source":["import torch\n","from tqdm.auto import tqdm\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=6e-5)\n","step=0\n","running_loss = 0.0\n","num_samples = 0\n","for epoch in range(2):\n","  print(\"Epoch:\", epoch)\n","  model.train()\n","  \n","  for idx, batch in enumerate(tqdm(train_dataloader)):\n","      # Reset the parameter gradients\n","      optimizer.zero_grad()\n","\n","      # Forward pass\n","      outputs = model(\n","          pixel_values=batch[\"pixel_values\"].to(device),\n","          mask_labels=[labels.to(device) for labels in batch[\"mask_labels\"]],\n","          class_labels=[labels.to(device) for labels in batch[\"class_labels\"]],\n","      )\n","\n","      # Backward propagation\n","      loss = outputs.loss\n","      loss.backward()\n","\n","      batch_size = batch[\"pixel_values\"].size(0)\n","      running_loss += loss.item()\n","      num_samples += batch_size\n","\n","      if idx % 100 == 0:\n","        print(\"Loss:\", running_loss/num_samples)\n","\n","      # Optimization\n","      optimizer.step()\n","      step += 1\n","      if step % 5000 == 0:\n","          model.eval()\n","          for idx, batch in enumerate(tqdm(test_dataloader)):\n","            if idx > 310:\n","              break\n","\n","            pixel_values = batch[\"pixel_values\"]\n","            \n","            # Forward pass\n","            with torch.no_grad():\n","                outputs = model(pixel_values=pixel_values.to(device))\n","\n","            # get original images\n","            original_images = batch[\"original_images\"]\n","            target_sizes = [(image.shape[0], image.shape[1]) for image in original_images]\n","            # predict segmentation maps\n","            predicted_segmentation_maps = preprocessor.post_process_semantic_segmentation(outputs,\n","                                                                                          target_sizes=target_sizes)\n","\n","            # get ground truth segmentation maps\n","            ground_truth_segmentation_maps = batch[\"original_segmentation_maps\"]\n","\n","            metric.add_batch(references=ground_truth_segmentation_maps, predictions=predicted_segmentation_maps)\n","          \n","          # NOTE this metric outputs a dict that also includes the mIoU per category as keys\n","          # so if you're interested, feel free to print them as well\n","          print(\"Mean IoU:\", metric.compute(num_labels = len(id2label), ignore_index = 0)['mean_iou'])\n","          torch.save(model.state_dict(), '/content/gdrive/MyDrive/flair-one/models/mask2former-swin-base-ade-semantic/mask2former-swin-base-ade-semantic')"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"private_outputs":true},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}