{"cells":[{"cell_type":"markdown","source":["# Inference of ensemble models on grid patch of 512*512 images\n","\n","---\n","\n","<a target=\"_blank\" href=\"https://colab.research.google.com/drive/1Yx7EePxXDbXlAFOjSIoxxpYVYgxJ5wc7\">\n","  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n","</a>\n","\n"],"metadata":{"id":"olA1iVq7GRff"}},{"cell_type":"markdown","source":["<img src=\"https://www.researchgate.net/publication/327983253/figure/fig1/AS:676937471102977@1538405878409/Left-Satellite-like-image-Right-Grid-cells-and-lines-with-different-colours.png\"  width=\"600\">\n","\n","\n","\n"],"metadata":{"id":"gT24bUtjHklQ"}},{"cell_type":"markdown","source":["## Install dependencies\n","\n","---\n","\n"],"metadata":{"id":"sYWkum4HGMZ3"}},{"cell_type":"code","source":["!pip install transformers\n","!pip install rasterio"],"metadata":{"id":"W_B--IKFGLJ3"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cjo5l3VwmT9X"},"outputs":[],"source":["import os\n","import cv2\n","import numpy as np\n","from glob import glob\n","from scipy.io import loadmat\n","import matplotlib.pyplot as plt\n","import matplotlib\n","import torch\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras import backend as K\n","from osgeo import gdal\n","import pandas as pd\n","import gc\n","import math\n","from transformers import TFSegformerForSemanticSegmentation, SegformerConfig"]},{"cell_type":"markdown","metadata":{"id":"fX8hBDIlwY8g"},"source":["## Connect to GCP\n","\n","---\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1NYFguWpmgyv"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"markdown","source":["## Get data\n","\n","---\n","\n"],"metadata":{"id":"z6_1yW7HGHuf"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"FzxiferEmg11"},"outputs":[],"source":["!unzip /content/gdrive/MyDrive/flair-one/data/flair-one_test.zip"]},{"cell_type":"markdown","source":["## Create folder to save results\n","\n","---\n","\n"],"metadata":{"id":"1cnw87PiH6HQ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"VUb4-YR_riwk"},"outputs":[],"source":["!mkdir preds\n","!mkdir mosaic\n","!mkdir /content/gdrive/MyDrive/predictions_mosaic\n","!mkdir predictions"]},{"cell_type":"markdown","metadata":{"id":"V3Rc26QRwuEx"},"source":["## Read Data\n","\n","---\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M27v1UKQmT9c"},"outputs":[],"source":["DATA_DIR = \"/content/test\"\n","\n","np.random.seed(123)\n","\n","def to_categorical(a):\n","  classes = np.arange(1,14)\n","  a_array = [(a == v) for v in classes]  #extract\n","  a = np.stack(a_array,axis=-1).astype(\"float\")  #stack\n","  return a\n","\n","def read_image(image_path, mask=False, resize = True):\n","    im = gdal.Open(image_path)\n","    if mask:\n","        image = im.ReadAsArray().transpose()\n","        image = np.where(np.isin(image, [19,13,14,15,16,17,18]), 13, image) - 1\n","        if resize:\n","             image = cv2.resize(image, (256,256))\n","    else:\n","        image = im.ReadAsArray().transpose().astype(np.float32)        \n","        image = image / 255.0       \n","        if resize:\n","             image = cv2.resize(image, (256,256))\n","    im = None\n","    return image"]},{"cell_type":"code","source":["img_paths = []\n","for dep in os.listdir(DATA_DIR):\n","    for zone in os.listdir(\"/\".join([DATA_DIR, dep])):\n","        for img in os.listdir(\"/\".join([DATA_DIR, dep, zone, \"img\"])):\n","            if img.__contains__(\"xml\"):\n","                continue\n","            img_path = \"/\".join([DATA_DIR, dep, zone, \"img\", img])\n","            img_paths.append(img_path)"],"metadata":{"id":"EH5CbIfQ_2K3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LyeNlkZ1wxGg"},"source":["## Create Patches\n","\n","---\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s0mkhs_jmT9d"},"outputs":[],"source":["patches = list(set([\"/\".join(img.split(\"/\")[3:5]) for img in img_paths]))\n","\n","# On crée les gros patchs grace au géoréférencement des petits patchs 512*512\n","PATH = \"/content/mosaic/\"\n","# vrts\n","for patch in patches:\n","    output = PATH + patch.replace('/', '_') + \".vrt\"\n","    img_list = [img for img in img_paths if patch in img]\n","    my_vrt = gdal.BuildVRT(output, img_list)\n","    my_vrt = None\n","\n","\n","# dictionnaire qui stocke les emprises de chaque image\n","emprise = {}\n","for patch in patches:\n","    img_list = [img for img in img_paths if patch in img]\n","    emprise2 = {}\n","    for img in img_list:\n","        file_name = img.split(\"/\")[-1][:-4]\n","        ds = gdal.Open(img)\n","        ulx, _,_, uly, _,_ = ds.GetGeoTransform()\n","        emprise2[file_name] = (ulx, uly)\n","    emprise[patch] = emprise2"]},{"cell_type":"markdown","metadata":{"id":"qTLGg5OIsak0"},"source":["## Import models\n","\n","---\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xJryjDnWyTUA"},"outputs":[],"source":["segformer_b0_5c_model_name = \"/content/gdrive/MyDrive/flair-one/models/segformer_b0_5c/segformer\"\n","segformer_b5_5c_model_name = \"/content/gdrive/MyDrive/flair-one/models/segformer_b5_5c/segformer\"\n","segformer_b0_rgb_model_name = \"/content/gdrive/MyDrive/flair-one/models/segformer_b0_rgb\"\n","segformer_b5_rgb_model_name = \"/content/gdrive/MyDrive/flair-one/models/segformer_b5_rgb\"\n","mask2former-large-ade-semantic_model_name = \"/content/gdrive/MyDrive/flair-one/models/mask2former-swin-large-ade-semantic/mask2former-large-ade-semantic.zip\"\n","mask2former-swin-base-ade-semantic_model_name = \"/content/gdrive/MyDrive/flair-one/models/mask2former-swin-base-ade-semantic/mask2former-swin-base-ade-semantic\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ukiceRhJTHhr"},"outputs":[],"source":["classes = ['None','building','pervious surface','impervious surface','bare soil','water','coniferous','deciduous','brushwood','vineyard','herbaceous vegetation','agricultural land','plowed land']\n","id2label = classes.to_dict()\n","label2id = {v: k for k, v in id2label.items()}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mqecKop3THhs"},"outputs":[],"source":["num_labels = len(id2label)\n","num_labels"]},{"cell_type":"markdown","metadata":{"id":"7L81QKe8yteo"},"source":["### Segformer  5C\n","\n","\n","---\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GUX_dukwxJM-"},"outputs":[],"source":["model_temp = TFSegformerForSemanticSegmentation.from_pretrained(\n","    \"nvidia/mit-b0\",\n","    num_labels=13\n",")\n","\n","new_config = model_temp.config\n","# print(new_config)\n","\n","new_config.num_channels = 5\n","\n","segformer_b0_5c_model = TFSegformerForSemanticSegmentation(new_config)\n","segformer_b0_5c_model.build(input_shape=(1,5,512,512))\n","\n","segformer_b0_5c_model.load_weights(segformer_b0_5c_model_name)\n","del model_temp"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uRStPKw0thRT"},"outputs":[],"source":["model_temp = TFSegformerForSemanticSegmentation.from_pretrained(\n","    \"nvidia/mit-b5\",\n","    num_labels=13\n",")\n","\n","new_config = model_temp.config\n","# print(new_config)\n","\n","new_config.num_channels = 5\n","\n","segformer_b5_5c_model = TFSegformerForSemanticSegmentation(new_config)\n","segformer_b5_5c_model.build(input_shape=(1,5,512,512))\n","\n","segformer_b5_5c_model.load_weights(segformer_b5_5c_model_name)\n","del model_temp"]},{"cell_type":"markdown","metadata":{"id":"PFqIEecaSQND"},"source":["### Segformers rgb\n","\n","---\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4lc6wEFnSV0G"},"outputs":[],"source":["from transformers import SegformerForSemanticSegmentation, SegformerFeatureExtractor\n","\n","segformer_b0_rgb_model = SegformerForSemanticSegmentation.from_pretrained(segformer_b0_rgb_model_name,\n","                                                                            num_labels=len(id2label),\n","                                                                            id2label=id2label,\n","                                                                            label2id=label2id)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YeBqh9RWsmAL"},"outputs":[],"source":["from transformers import SegformerForSemanticSegmentation, SegformerFeatureExtractor\n","\n","segformer_b5_rgb_model = SegformerForSemanticSegmentation.from_pretrained(segformer_b5_rgb_model_name,\n","                                                                            num_labels=len(id2label),\n","                                                                            id2label=id2label,\n","                                                                            label2id=label2id)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MbzUivs3SV4b"},"outputs":[],"source":["segformer_rgb_feature_extractor = SegformerFeatureExtractor(ignore_index=0, reduce_labels=False, do_resize=False, do_rescale=False, do_normalize=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_s9IAmmyz0Jp"},"outputs":[],"source":["\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device\n","\n","segformer_b0_rgb_model = segformer_b0_rgb_model.to(device)\n","segformer_b5_rgb_model = segformer_b5_rgb_model.to(device)"]},{"cell_type":"markdown","metadata":{"id":"XHPRM0Z-N0TO"},"source":["## Mask2former\n","\n","---\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P3E7gQZrQz1e"},"outputs":[],"source":["from transformers import MaskFormerImageProcessor\n","\n","# Create a preprocessor\n","preprocessor = MaskFormerImageProcessor(ignore_index=0, reduce_labels=False, do_resize=False, do_rescale=False, do_normalize=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rETyEzx-wUef"},"outputs":[],"source":["# Replace the head of the pre-trained model\n","\n","from transformers import Mask2FormerForUniversalSegmentation\n","mask2former_swin_large_ade_semantic_model = Mask2FormerForUniversalSegmentation.from_pretrained(\"facebook/mask2former-swin-large-cityscapes-semantic\",\n","                                                          id2label=id2label,\n","                                                          ignore_mismatched_sizes=True)\n","mask2former_swin_large_ade_semantic_model.load_state_dict(torch.load(mask2former-large-ade-semantic_model_name))   \n","mask2former_swin_large_ade_semantic_model = mask2former_swin_large_ade_semantic_model.to(device)  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lfG9VBWscWpp"},"outputs":[],"source":["from transformers import Mask2FormerForUniversalSegmentation\n","\n","# Replace the head of the pre-trained model\n","mask2former_swin_base_ade_semantic_model = Mask2FormerForUniversalSegmentation.from_pretrained(\"facebook/mask2former-swin-base-IN21k-ade-semantic\",\n","                                                          id2label=id2label,\n","                                                          ignore_mismatched_sizes=True)\n","mask2former_swin_base_ade_semantic_model.load_state_dict(torch.load(mask2former-swin-base-ade-semantic_model_name))  \n","mask2former_swin_base_ade_semantic_model = mask2former_swin_base_ade_semantic_model.to(device)"]},{"cell_type":"markdown","metadata":{"id":"Yw8o2FUozqAA"},"source":["## Inference sur la grille\n","\n","---\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LZvq8PvcmT9e"},"outputs":[],"source":["def pred_tif_sf(rgb_path, stride = (256,256), size = 512, tile_path = \"/content/preds/\", output_name = \"all\"):\n","    stride_x, stride_y = stride\n","    rgb = gdal.Open(rgb_path)\n","    gt = list(rgb.GetGeoTransform())\n","    originY = gt[3]\n","    originX = gt[0]\n","    width, height = rgb.RasterXSize, rgb.RasterYSize\n","\n","    \n","    for row in range(stride_y, height, size): #range(0, height, stride)\n","        # print(row)\n","        res = []\n","        res_5C = []\n","        if row + size > height:\n","            break\n","        for col in range(stride_x, width, size):\n","            if col + size > width:\n","                break\n","            arr = rgb.ReadAsArray(xoff=col, yoff=row, xsize=size, ysize=size).astype(np.float32)\n","            arr = arr/255.0\n","            arr = np.expand_dims(arr, axis = 0) # 1,256,256,5\n","            res.append(arr)\n","\n","\n","        res = np.concatenate(res, axis = 0)\n","        for channel,avg,std in zip(\n","            [0,1,2,3,4],\n","            [0.44050665, 0.45704361, 0.42254708, 0.40987858, 0.06875153],\n","            [0.20264351, 0.1782405 , 0.17575739, 0.15510736, 0.11867123]):\n","\n","            res[:,channel,:,:] = (res[:,channel,:,:] - avg)/std\n","        res_rgb = res[:,:3,:,:] \n","\n","\n","        ######################################################################################################\n","        # MASK2FORMER large\n","        results = []\n","        for image in res_rgb:\n","            pixel_values = preprocessor (image, return_tensors=\"pt\").pixel_values.to(device)\n","            with torch.no_grad():\n","                outputs = mask2former_swin_large_ade_semantic_model(pixel_values=pixel_values)\n","            target_sizes = [(512, 512)]\n","\n","            pred_mask2former_ade_large_tp =  preprocessor.post_process_semantic_segmentation(outputs, target_sizes=target_sizes)\n","            pred_mask2former_ade_large_tp = np.array(pred_mask2former_ade_large_tp[0].cpu().detach().numpy()) - 1 \n","\n","            class_queries_logits = outputs.class_queries_logits  # [batch_size, num_queries, num_classes+1]\n","            masks_queries_logits = outputs.masks_queries_logits  # [batch_size, num_queries, height, width]\n","\n","            # Scale back to preprocessed image size - (384, 384) for all models\n","            masks_queries_logits = torch.nn.functional.interpolate(\n","                masks_queries_logits, size=(384, 384), mode=\"bilinear\", align_corners=False)\n","\n","            # Remove the null class `[..., :-1]`\n","            masks_classes = class_queries_logits.softmax(dim=-1)[..., :-1]\n","            masks_probs = masks_queries_logits.sigmoid()  # [batch_size, num_queries, height, width]\n","\n","            # Semantic segmentation logits of shape (batch_size, num_classes, height, width)\n","            segmentation = torch.einsum(\"bqc, bqhw -> bchw\", masks_classes, masks_probs)\n","            batch_size = class_queries_logits.shape[0]\n","\n","            # Resize logits and compute semantic segmentation maps\n","            if target_sizes is not None:\n","                semantic_segmentation = []\n","                for idx in range(batch_size):\n","                    resized_logits = torch.nn.functional.interpolate(segmentation[idx].unsqueeze(dim=0), size=target_sizes[idx], mode=\"bilinear\", align_corners=False\n","                        )\n","                    semantic_map = resized_logits[0]   #.argmax(dim=0)\n","                    semantic_segmentation.append(semantic_map)\n","            pred_mask2former_ade_large =  semantic_segmentation # preprocessor.post_process_semantic_segmentation(outputs, target_sizes=target_sizes)\n","            pred_mask2former_ade_large = np.array(pred_mask2former_ade_large[0].cpu().detach().numpy())\n","            pred_mask2former_ade_large = tf.transpose(np.expand_dims(pred_mask2former_ade_large, axis = 0), perm=[0,2,3,1])\n","            results.append(np.squeeze(pred_mask2former_ade_large))\n","        pred_mask2former_ade_large = results\n","\n","\n","        ######################################################################################################\n","        # MASK2FORMER base\n","        results = []\n","        for image in res_rgb:\n","            pixel_values = preprocessor (image, return_tensors=\"pt\").pixel_values.to(device)\n","            with torch.no_grad():\n","                outputs = mask2former_swin_base_ade_semantic_model(pixel_values=pixel_values)\n","            target_sizes = [(512, 512)]\n","\n","            pred_mask2former_ade_base_tp =  preprocessor.post_process_semantic_segmentation(outputs, target_sizes=target_sizes)\n","            pred_mask2former_ade_base_tp = np.array(pred_mask2former_ade_base_tp[0].cpu().detach().numpy()) - 1 \n","\n","            class_queries_logits = outputs.class_queries_logits  # [batch_size, num_queries, num_classes+1]\n","            masks_queries_logits = outputs.masks_queries_logits  # [batch_size, num_queries, height, width]\n","\n","            # Scale back to preprocessed image size - (384, 384) for all models\n","            masks_queries_logits = torch.nn.functional.interpolate(\n","                masks_queries_logits, size=(384, 384), mode=\"bilinear\", align_corners=False)\n","\n","            # Remove the null class `[..., :-1]`\n","            masks_classes = class_queries_logits.softmax(dim=-1)[..., :-1]\n","            masks_probs = masks_queries_logits.sigmoid()  # [batch_size, num_queries, height, width]\n","\n","            # Semantic segmentation logits of shape (batch_size, num_classes, height, width)\n","            segmentation = torch.einsum(\"bqc, bqhw -> bchw\", masks_classes, masks_probs)\n","            batch_size = class_queries_logits.shape[0]\n","\n","            # Resize logits and compute semantic segmentation maps\n","            if target_sizes is not None:\n","                semantic_segmentation = []\n","                for idx in range(batch_size):\n","                    resized_logits = torch.nn.functional.interpolate(segmentation[idx].unsqueeze(dim=0), size=target_sizes[idx], mode=\"bilinear\", align_corners=False\n","                        )\n","                    semantic_map = resized_logits[0]   #.argmax(dim=0)\n","                    semantic_segmentation.append(semantic_map)\n","            pred_mask2former_ade_base =  semantic_segmentation # preprocessor.post_process_semantic_segmentation(outputs, target_sizes=target_sizes)\n","            pred_mask2former_ade_base = np.array(pred_mask2former_ade_base[0].cpu().detach().numpy())\n","            pred_mask2former_ade_base = tf.transpose(np.expand_dims(pred_mask2former_ade_base, axis = 0), perm=[0,2,3,1])\n","            results.append(np.squeeze(pred_mask2former_ade_base))\n","        pred_mask2former_ade_base = results\n","\n","\n","        ######################################################################################################\n","        # Segformer b0 rgb\n","        results = []\n","        for image in res_rgb:\n","            pixel_values = segformer_rgb_feature_extractor(image, return_tensors=\"pt\").pixel_values.to(device)\n","            outputs_segformer_b0_rgb = segformer_b0_rgb_model(pixel_values=pixel_values)# logits are of shape (batch_size, num_labels, height/4, width/4)\n","            pred_segformer_b0_rgb = outputs_segformer_b0_rgb.logits.cpu().detach().numpy()\n","            pred_segformer_b0_rgb = tf.image.resize(tf.transpose(pred_segformer_b0_rgb, perm=[0,2,3,1]), size = [512,512], method=\"bilinear\") # resize to 512*512  \n","            results.append(np.squeeze(pred_segformer_b0_rgb))\n","        pred_segformer_b0_rgb = results\n","\n","\n","        ######################################################################################################\n","        # Segformer b5 rgb\n","        results = []\n","        for image in res_rgb:\n","            pixel_values = segformer_rgb_feature_extractor(image, return_tensors=\"pt\").pixel_values.to(device)\n","            outputs_segformer_b5_rgb = segformer_b5_rgb_model(pixel_values=pixel_values)# logits are of shape (batch_size, num_labels, height/4, width/4)\n","            pred_segformer_b5_rgb = outputs_segformer_b5_rgb.logits.cpu().detach().numpy()\n","            pred_segformer_b5_rgb = tf.image.resize(tf.transpose(pred_segformer_b5_rgb, perm=[0,2,3,1]), size = [512,512], method=\"bilinear\") # resize to 512*512\n","            results.append(np.squeeze(pred_segformer_b5_rgb))\n","        pred_segformer_b5_rgb = results\n","\n","\n","        ######################################################################################################\n","        # Segformer b0 5c\n","        pred_segformer_b0_5c = segformer_b0_5c_model.predict(res, batch_size=1)\n","        pred_segformer_b0_5c = list(pred_segformer_b0_5c.values())[0]\n","        pred_segformer_b0_5c = pred_segformer_b0_5c[:,[12,0,1,2,3,4,5,6,7,8,9,10,11],:,:]\n","        pred_segformer_b0_5c = tf.image.resize(tf.transpose(pred_segformer_b0_5c, perm=[0,2,3,1]), size = [512,512], method=\"bilinear\") # resize to 512*512\n","\n","\n","        ######################################################################################################\n","        # Segformer b5 5c\n","        pred_segformer_b5_5c = segformer_b5_5c_model.predict(res, batch_size=1)\n","        pred_segformer_b5_5c = list(pred_segformer_b5_5c.values())[0]\n","        pred_segformer_b5_5c = pred_segformer_b5_5c[:,[12,0,1,2,3,4,5,6,7,8,9,10,11],:,:]\n","        pred_segformer_b5_5c = tf.image.resize(tf.transpose(pred_segformer_b5_5c, perm=[0,2,3,1]), size = [512,512], method=\"bilinear\") # resize to 512*512\n","\n","\n","        ######################################################################################################\n","        # Mean predictio,\n","        preds = np.mean(np.array([pred_segformer_b0_rgb,\n","                                  pred_segformer_b5_5c,\n","                                  pred_segformer_b5_rgb,\n","                                  pred_segformer_b0_5c,\n","                                  pred_mask2former_ade_large,\n","                                  pred_mask2former_ade_base,\n","                                ]), axis = 0)\n","\n","        preds = [np.argmax(preds[index,:,:,:], axis = -1).transpose((0,1)) for index in range(preds.shape[0])]\n","        preds = np.array(preds)-1\n","        #print(np.array(preds).shape)\n","\n","        outfile = tile_path + str(row) + \".tif\"\n","        driver = gdal.GetDriverByName('GTiff')\n","        dataset = driver.Create(outfile, col + size, size, 1, gdal.GDT_Byte)\n","        preds = np.hstack(preds)\n","        dataset.GetRasterBand(1).WriteArray(preds + 1) # +1 pour reconnaitre les zones non predites(=0)\n","\n","        # follow code is adding GeoTranform and Projection\n","        gt[3] = originY + row * gt[5]\n","        gt[0] = originX + stride_x * gt[1]\n","        proj = rgb.GetProjection() #you can get from a exsited tif or import \n","        dataset.SetGeoTransform(gt)\n","        dataset.SetProjection(proj)\n","        dataset.FlushCache()\n","        dataset=None\n","        _ = gc.collect()\n","    \n","    rgb =  None\n","    #print( [\"\".join([tile_path,i]) for i in os.listdir(tile_path) if \"al\" not in i and \"rf\" not in i])\n","    ds = gdal.BuildVRT(srcDSOrSrcDSTab = [\"\".join([tile_path,i]) for i in os.listdir(tile_path) if \"al\" not in i and \"rf\" not in i], destName = tile_path + output_name + \".vrt\")\n","    ds = None\n","\n","    ds = gdal.Warp(srcDSOrSrcDSTab=tile_path + output_name + \".vrt\", \n","    destNameOrDestDS=tile_path + output_name + \".tif\", \n","    outputType  = gdal.gdalconst.GDT_Byte, \n","    multithread =True, srcSRS = \"+proj=lcc +lat_1=49 +lat_2=44 +lat_0=46.5 +lon_0=3 +x_0=700000 +y_0=6600000 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs\",\n","    outputBounds = (originX, originY + height * gt[5], originX + width * gt[1], originY))\n","    ds = None\n","\n","    # effacer tuiles\n","    tiles = [os.remove(tile_path + i) for i in os.listdir(tile_path) if \"al\" not in i and \"rf\" not in i]\n","    return \n","\n","\n","\n","def merge_all(file_list, output):\n","    \n","    driver = gdal.GetDriverByName('GTiff')\n","    ds1 = gdal.Open(file_list[0])\n","\n","    dataset = driver.Create(output, ds1.RasterXSize, ds1.RasterYSize, 1, gdal.GDT_Byte)\n","        \n","    step = int(ds1.RasterXSize/10)\n","    for col in range(0, ds1.RasterXSize, step):\n","        if col + step > ds1.RasterXSize:\n","            step = ds1.RasterXSize - col\n","        #print(col)\n","        arrs = np.concatenate(list(map(lambda x: np.expand_dims(gdal.Open(x).ReadAsArray(xoff = col, yoff= 0, xsize= step), axis = 2), file_list)), axis = -1)\n","        \n","        arr2 = np.concatenate([np.expand_dims((arrs == i).sum(axis = 2), axis = 2) for i in range(1,14)], axis = -1)\n","        arr2 = np.argmax(arr2, axis = -1)\n","\n","        dataset.GetRasterBand(1).WriteArray(arr2, xoff = col, yoff= 0)\n","        dataset.FlushCache()\n","        gc.collect()\n","\n","\n","    dataset.SetGeoTransform(ds1.GetGeoTransform())\n","    dataset.SetProjection(ds1.GetProjection())\n","    \n","    dataset=ds1 = None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a5J51nq-b-3d"},"outputs":[],"source":["len(patches)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fO2hmqgSkdzw"},"outputs":[],"source":["import os\n","subdirs = [x[0] for x in os.walk('/content/gdrive/MyDrive/predictions_mosaic/')]\n","print(len(subdirs))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dv7XagPXmT9f"},"outputs":[],"source":["\n","PATH = \"/content/mosaic/\"\n","vrts = [PATH + file_name for file_name in os.listdir(PATH)]\n","PRED_PATH = \"/content/gdrive/MyDrive/predictions_mosaic/\"\n","\n","\n","for i, patch in enumerate(patches):\n","    check = PRED_PATH + patch.replace('/', '_')\n","    print(\"patch: \", patch,\" \" , i)\n","    if check in subdirs:\n","        print('pass')\n","        pass\n","        \n","    else:\n","        # print(\"patch: \", patch,\" \" , i)\n","        path = PRED_PATH + patch.replace('/', '_') + '/' \n","        if not os.path.exists(path):\n","            os.makedirs(path)\n","        # predictions \n","        vrt_path = PATH + patch.replace('/', '_') + '.vrt'\n","        a = 0\n","        for i in range(4):\n","            for j in range(4):\n","                a = a+1\n","                pred_tif_sf(rgb_path = vrt_path,\n","                stride = (i*128, j*128), size= 512, \n","                tile_path = path, output_name = \"all\" + str(a))\n","        \n","        \n","        # merge\n","        tiles = os.listdir(path)\n","        tiles = [path + tile for tile in tiles if \".tif\"  in tile and \"all\" in tile]\n","        merge_all(file_list = tiles, output = path + \"final.tif\")\n","        #clear_output()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RvWGcDVemT9f"},"outputs":[],"source":["# unpatch\n","PATH2 = '/content/predictions/'\n","\n","for i, patch in enumerate(patches):\n","    path = PRED_PATH + patch.replace('/', '_') + '/'\n","    print(patch, path)\n","    bigimg = gdal.Open(path + 'final.tif')\n","    ULX,_,_,ULY,_,_ = bigimg.GetGeoTransform()\n","    for img in emprise[patch].keys():\n","        ulx, uly = emprise[patch][img]\n","        arr = bigimg.ReadAsArray(\n","            xoff = round((ulx-ULX)/0.2),\n","            yoff = abs(round((uly-ULY)/0.2)),\n","            xsize = 512, ysize = 512\n","        )\n","\n","        driver = gdal.GetDriverByName('GTiff')\n","        dataset = driver.Create(PATH2 + img.replace('IMG', 'PRED') + '.tif', 512, 512, 1, gdal.GDT_Byte)\n","        dataset.GetRasterBand(1).WriteArray(arr)\n","        proj = bigimg.GetProjection() #you can get from a exsited tif or import \n","        dataset.SetProjection(proj)\n","        dataset.FlushCache()\n","        dataset=None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QJb5WOjSVRfo"},"outputs":[],"source":["!zip -r /content/gdrive/MyDrive/flair-one/segformers_mask2former_grid.zip /content/predictions"]}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm","private_outputs":true},"gpuClass":"standard","kernelspec":{"display_name":"tf","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]"},"vscode":{"interpreter":{"hash":"094714fe796e6c1218ad20991b7e5262c49457f2d9464e34a3ce061e080d5275"}}},"nbformat":4,"nbformat_minor":0}